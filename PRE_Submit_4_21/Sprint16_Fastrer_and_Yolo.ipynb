{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "ylnmpSa4F3nt",
    "outputId": "916070d0-c0ae-4d46-ce30-fdd5b77b3d9f"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8OYDo7eXy5yf"
   },
   "source": [
    "# Sprint16 公開されている実装を動かす\n",
    "READMEを参考に上記実装を動かしてください。 \n",
    "Faster R-CNN[1]の実装を動かします。\n",
    "\n",
    "[1]Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. In: Advances in neural information processing systems. (2015) 91–99\n",
    "\n",
    "https://arxiv.org/pdf/1506.01497.pdf\n",
    "\n",
    "以下のものを使用してください。Kerasを使用した実装です。\n",
    "\n",
    "duckrabbits/ObjectDetection at master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UWnrqB4CHrq7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fLP_uejLzCKM"
   },
   "source": [
    "## 【問題1】学習と推定\n",
    "READMEを参考に上記実装を動かしてください。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "guktGFg7zIky"
   },
   "source": [
    "\n",
    "\n",
    "### (README)\n",
    "Kerasを利用したFaster R-CNN実装により物体検出を行います。\n",
    "\n",
    "データセットの用意\n",
    "Kaggleからダウンロードしたthe-simpsons-characters-dataset.zipを解凍します。  \n",
    "simpsons_dataset.zipを解凍します。  \n",
    "解凍したsimpsons_datasetフォルダをプロジェクトフォルダ配下に格納します。  \n",
    "annotation.txtもプロジェクトフォルダ配下に格納します。   \n",
    "（kaggleからダウンロードしたannotation.txtではなく、ここにあるannotation.txtを使いましょう）\n",
    "\n",
    "### 学習\n",
    "\n",
    "以下のコマンドを実行してください。\n",
    "\n",
    "python train.py -p annotation.txt\n",
    "\n",
    "以下のように設定ファイルのパスが出力されるので記録してください。推定時に使います。\n",
    "\n",
    "\"\"\" path to config file : ./save/train_20190309-220050_config.pickle \"\"\"\n",
    "\n",
    "エポック数なども引数で変更できます。詳細はtrain.pyを参照してください。\n",
    "\n",
    "\n",
    "### 推定\n",
    "\n",
    "以下のコマンドを実行してください。\n",
    "\n",
    "python predict.py -i [推定したい画像を入れたディレクトリのパス] -c [学習時に保存された設定ファイルのパス]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RpjkluwRzVGS"
   },
   "source": [
    "### jupyter環境で実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TAWSG2H7zMuR"
   },
   "outputs": [],
   "source": [
    "#kaggle　のシンプソンズデータセットをインストール\n",
    "# !pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M4Z_-vzOzgdJ"
   },
   "outputs": [],
   "source": [
    "# ls -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b40vOMGezj07"
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/　へアクセスし、ログイン後に右上のプロフィール画像をクリック、さらに\"My Account\"をクリックする\n",
    "# このサイトの「API」項目にある\"Create New API Token\"をクリックすると、kaggle.jsonファイルが自動的にダウンロードされる\n",
    "# ローカルで、kaggle.json（ダウンロードフォルダにあるはず）をエディターで開く。\n",
    "# このセルの以下のコードにある token = {'username':'***','key':'***'} における「***」部分を、\n",
    "# ダウンロードしたkaggle.jsonを参照して書き換え、このセルを実行する\n",
    "\n",
    "# import json\n",
    "\n",
    "# token = {'username':'kaorisugi','key':'45dfd98b8e531a606d8922234e841c6d'}\n",
    "# with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
    "#     json.dump(token, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mLU2rLtMzmd8"
   },
   "outputs": [],
   "source": [
    "# !chmod 600 /content/.kaggle/kaggle.json\n",
    "# !cp /content/.kaggle/kaggle.json /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l_DV3BZIzq0l"
   },
   "outputs": [],
   "source": [
    "# simpsonデータセットのAPIcommandを叩いてデータセットをダウンロードする\n",
    "# APIcommandはこちらのサイトに。https://www.kaggle.com/alexattia/the-simpsons-characters-dataset\n",
    "\n",
    "# !kaggle datasets download -d alexattia/the-simpsons-characters-dataset\n",
    "\n",
    "# !unzip /content/the-simpsons-characters-dataset.zip  -d Simpsons\n",
    "# # train用のデータセットを解凍\n",
    "\n",
    "# !unzip /content/Simpsons/simpsons_dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S-zBMIdo0N_6"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/duckrabbits/ObjectDetection.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pQ0V94ga0RTf"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.chdir('/content/ObjectDetection')\n",
    "# print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/user/programing/diveintocode-ml/Sprint16'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md        \u001b[34mmodel\u001b[m\u001b[m/           \u001b[34msave\u001b[m\u001b[m/            train.py\r\n",
      "annotation.txt   predict.py       \u001b[34msimpson_testset\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/user/programing/diveintocode-ml/Sprint16/ObjectDetection\n"
     ]
    }
   ],
   "source": [
    "cd ObjectDetection/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KCjdfymQ0ZWP"
   },
   "outputs": [],
   "source": [
    "# ! python train.py -p annotation.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KPy31MkY0eSj"
   },
   "outputs": [],
   "source": [
    "# 修正箇所\n",
    "# tf ==> channels_last\n",
    "# K.image_dim_ordering() ==> K.image_data_format\n",
    "# gitのanotation.txtとkaggle_dataのファイル 名を合わせた\n",
    "# その他ファイル の移動など"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python train.py -p annotation.txt#実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】コードリーディング\n",
    "上記実装のコードリーディングを行ってください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MFOk31dW0hOD"
   },
   "source": [
    "### RPNを実現しているコードはどこか\n",
    "論文によると、今までのFast_rcnnなどはResion Proposalの計算時間が課題だった。  \n",
    "今回FasterではRPNを採用することで高速化している。（一番の売り）\n",
    "### RoIプーリングを実現しているコードはどこか\n",
    "物体検出の際、RPNから出力されるFeature map を　その後の分類作業に渡し、情報共有することで高速化を実現している　。  \n",
    "情報共有の際にデータの形状を合わせる必要があり、それを実施しているのがRoIプーリング（これも売りの一つ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XyKrGfFt1DkM"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\"\"\"Faster_rcnnのモデル\"\"\"\n",
    "from model import faster_rcnn\n",
    "\n",
    "from model import config, data_generators\n",
    "from model.parser import get_data\n",
    "\n",
    "\"\"\"ROIプーリングのモデル\"\"\"\n",
    "import model.roi_helpers as roi_helpers\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.utils import generic_utils\n",
    "\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "parser = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter, description='Object Detection')\n",
    "parser.add_argument(\"-p\", \"--path\", default=None, help=\"path to annotation file\")\n",
    "parser.add_argument(\"--save_dir\", default=\"./save\", help=\"path to save directory\")\n",
    "parser.add_argument('--n_epochs', default=10, type=int, metavar='N',\n",
    "                    help='number of epochs')\n",
    "parser.add_argument('--n_iters', default=100, type=int, metavar='N',\n",
    "                    help='number of iterations')\n",
    "parser.add_argument('--horizontal_flips', action='store_true',\n",
    "                    help='augument with horizontal flips (Default:False)')\n",
    "parser.add_argument('--vertical_flips', action='store_true',\n",
    "                    help='augument with horizontal flips (Default:False)')\n",
    "parser.add_argument('--rot_90', action='store_true',\n",
    "                    help='augument with 90 degree rotations (Default:False)')\n",
    "\n",
    "def main():\n",
    "    args = parser.parse_args()\n",
    "    time_stamp = \"{0:%Y%m%d-%H%M%S}\".format(datetime.now())\n",
    "    save_name = os.path.join(args.save_dir, \"train_{}\".format(time_stamp))\n",
    "\n",
    "    if not(os.path.isdir(args.save_dir)):\n",
    "        os.makedirs(args.save_dir)\n",
    "    if args.path == None:\n",
    "        raise OSError(\"path to annotation file must be required.\")\n",
    "    C = config.Config()\n",
    "    C.config_filename = save_name + \"_config.pickle\"\n",
    "    C.model_path = save_name + \"_model.hdf5\"\n",
    "    C.use_horizontal_flips = bool(args.horizontal_flips)\n",
    "    C.use_vertical_flips = bool(args.vertical_flips)\n",
    "    C.rot_90 = bool(args.rot_90)\n",
    "    all_imgs, classes_count, class_mapping = get_data(args.path)\n",
    "    C.class_mapping = class_mapping\n",
    "\n",
    "    with open(C.config_filename, 'wb') as config_f:\n",
    "        pickle.dump(C,config_f)\n",
    "        print(\"-------------------------------\")\n",
    "        print('path to config file : {}'.format(C.config_filename))\n",
    "        print(\"-------------------------------\")\n",
    "\n",
    "    train_imgs = [s for s in all_imgs if s['imageset'] == 'trainval']\n",
    "    val_imgs = [s for s in all_imgs if s['imageset'] == 'test']\n",
    "\n",
    "    data_gen_train = data_generators.get_anchor_gt(train_imgs, classes_count, C, K.image_dim_ordering(), mode='train')\n",
    "    data_gen_val = data_generators.get_anchor_gt(val_imgs, classes_count, C, K.image_dim_ordering(), mode='val')\n",
    "    \n",
    "    \"\"\"fasterのモデルからrpnとclassifier等のモデルをもらっている\"\"\"\n",
    "    model_rpn, model_classifier, model_all = faster_rcnn.get_model(C, classes_count)\n",
    "    \n",
    "    losses = np.zeros((args.n_iters, 5))\n",
    "    rpn_accuracy_rpn_monitor, rpn_accuracy_for_epoch = [], []\n",
    "\n",
    "    best_loss = np.Inf\n",
    "\n",
    "    with open('out.csv', 'w') as f:\n",
    "        f.write('Accuracy,RPN classifier,RPN regression,Detector classifier,Detector regression,Total')\n",
    "        f.write('\\t')\n",
    "\n",
    "    iter_num = 0\n",
    "\n",
    "    t0 = start_time = time.time()\n",
    "    try:\n",
    "        for epoch_num in range(args.n_epochs):\n",
    "            progbar = generic_utils.Progbar(args.n_iters)\n",
    "            print('Epoch {}/{}'.format(epoch_num + 1, args.n_epochs))\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    if len(rpn_accuracy_rpn_monitor) == args.n_iters and C.verbose:\n",
    "                        mean_overlapping_bboxes = float(sum(rpn_accuracy_rpn_monitor))/len(rpn_accuracy_rpn_monitor)\n",
    "                        rpn_accuracy_rpn_monitor = []\n",
    "                        print('Average number of overlapping bounding boxes from RPN = {} for {} previous iterations'.format(mean_overlapping_bboxes, args.n_iters))\n",
    "                        if mean_overlapping_bboxes == 0:\n",
    "                            print('RPN is not producing bounding boxes that overlap the ground truth boxes. Check RPN settings or keep training.')\n",
    "                    X, Y, img_data = next(data_gen_train)\n",
    "                    \n",
    "                    \"\"\"Resion Proposalの学習、推定、ROIプーリングをしている。\"\"\"\n",
    "                    loss_rpn = model_rpn.train_on_batch(X, Y)\n",
    "                    P_rpn = model_rpn.predict_on_batch(X)\n",
    "                    R = roi_helpers.rpn_to_roi(P_rpn[0], P_rpn[1], C, K.image_dim_ordering(), use_regr=True, overlap_thresh=0.7, max_boxes=300)\n",
    "                    \n",
    "                    # note: calc_iou converts from (x1,y1,x2,y2) to (x,y,w,h) format\n",
    "                    X2, Y1, Y2 = roi_helpers.calc_iou(R, img_data, C, class_mapping)\n",
    "                    \n",
    "\n",
    "\n",
    "                    neg_samples = np.where(Y1[0, :, -1] == 1)\n",
    "                    pos_samples = np.where(Y1[0, :, -1] == 0)\n",
    "                    if len(neg_samples) > 0:\n",
    "                        neg_samples = neg_samples[0]\n",
    "                    else:\n",
    "                        neg_samples = []\n",
    "\n",
    "                    if len(pos_samples) > 0:\n",
    "                        pos_samples = pos_samples[0]\n",
    "                    else:\n",
    "                        pos_samples = []\n",
    "\n",
    "                    rpn_accuracy_rpn_monitor.append(len(pos_samples))\n",
    "                    rpn_accuracy_for_epoch.append((len(pos_samples)))\n",
    "                    if len(pos_samples) < C.num_rois//2:\n",
    "                        selected_pos_samples = pos_samples.tolist()\n",
    "                    else:\n",
    "                        selected_pos_samples = np.random.choice(pos_samples, C.num_rois//2, replace=False).tolist()\n",
    "                    try:\n",
    "                        selected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=False).tolist()\n",
    "                    except:\n",
    "                        selected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=True).tolist()\n",
    "\n",
    "                    sel_samples = selected_pos_samples + selected_neg_samples\n",
    "\n",
    "                    loss_class = model_classifier.train_on_batch([X, X2[:, sel_samples, :]], [Y1[:, sel_samples, :], Y2[:, sel_samples, :]])\n",
    "\n",
    "                    if iter_num == args.n_iters:\n",
    "                        loss_rpn_cls = np.mean(losses[:, 0])\n",
    "                        loss_rpn_regr = np.mean(losses[:, 1])\n",
    "                        loss_class_cls = np.mean(losses[:, 2])\n",
    "                        loss_class_regr = np.mean(losses[:, 3])\n",
    "                        class_acc = np.mean(losses[:, 4])\n",
    "\n",
    "                        mean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)\n",
    "                        rpn_accuracy_for_epoch = []\n",
    "\n",
    "                        if C.verbose:\n",
    "                            print('Mean number of bounding boxes from RPN overlapping ground truth boxes: {}'.format(mean_overlapping_bboxes))\n",
    "                            print('Classifier accuracy for bounding boxes from RPN: {}'.format(class_acc))\n",
    "                            print('Loss RPN classifier: {}'.format(loss_rpn_cls))\n",
    "                            print('Loss RPN regression: {}'.format(loss_rpn_regr))\n",
    "                            print('Loss Detector classifier: {}'.format(loss_class_cls))\n",
    "                            print('Loss Detector regression: {}'.format(loss_class_regr))\n",
    "                            print('Elapsed time: {}[s]'.format(time.time() - start_time))\n",
    "\n",
    "                        target_text_file = open('out.csv', 'a')\n",
    "                        target_text_file.write('{},{},{},{},{},{}'.format(class_acc, loss_rpn_cls,\n",
    "                                                loss_rpn_regr, loss_class_cls, loss_class_regr,\n",
    "                                                loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr))\n",
    "                        target_text_file.write('\\t')\n",
    "\n",
    "                        curr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr\n",
    "                        iter_num = 0\n",
    "                        start_time = time.time()\n",
    "\n",
    "                        if curr_loss < best_loss:\n",
    "                            if C.verbose:\n",
    "                                print('Total loss decreased from {} to {}, saving weights'.format(best_loss,curr_loss))\n",
    "                            best_loss = curr_loss\n",
    "                            model_all.save_weights(C.model_path)\n",
    "                        break\n",
    "\n",
    "                    losses[iter_num, 0] = loss_rpn[1]\n",
    "                    losses[iter_num, 1] = loss_rpn[2]\n",
    "                    losses[iter_num, 2] = loss_class[1]\n",
    "                    losses[iter_num, 3] = loss_class[2]\n",
    "                    losses[iter_num, 4] = loss_class[3]\n",
    "                    iter_num += 1\n",
    "\n",
    "                    progbar.update(iter_num, [('rpn_cls', np.mean(losses[:iter_num, 0])), ('rpn_regr', np.mean(losses[:iter_num, 1])),\n",
    "                                              ('detector_cls', np.mean(losses[:iter_num, 2])), ('detector_regr', np.mean(losses[:iter_num, 3]))])\n",
    "\n",
    "                except Exception as e:\n",
    "                    print('Exception: {}'.format(e))\n",
    "                    continue\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        t1 = time.time()\n",
    "        print('\\nIt took {:.2f}s'.format(t1-t0))\n",
    "        sys.exit('Keyboard Interrupt')\n",
    "\n",
    "    print(\"training is done\")\n",
    "    print(\"-------------------------------\")\n",
    "    print('path to config file : {}'.format(C.config_filename))\n",
    "    print(\"-------------------------------\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aIPZ2Ylm1KwE"
   },
   "source": [
    "### 【問題3】学習済みの重みによる推定\n",
    "学習済みの重みを使い推定を行う方法がREADME.mdのQuick Startの項に記載されています。  \n",
    "まずはこの通りにして各自何かしらの画像や動画に対して検出を行ってください。  \n",
    "出力結果を課題の一部として提出してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yt68I_p_1GuH"
   },
   "source": [
    "# 3.YOLOv3\n",
    "シンプソンズのデータセットをFaster R-CNN以外の手法で学習・推定を行います。YOLOv3[2]のKeras実装を使います。\n",
    "\n",
    "qqwweee/keras-yolo3: A Keras implementation of YOLOv3 (Tensorflow backend)\n",
    "\n",
    "[2]Jeseph Redmon, Ali Farhadi. YOLOv3: An Incremental Improvement\n",
    "\n",
    "https://pjreddie.com/media/files/papers/YOLOv3.pdf  \n",
    "https://github.com/qqwweee/keras-yolo3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "a_LFyBU7z9Pm",
    "outputId": "ecdfc71e-ecfa-4177-97ea-0c5e70e48f26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/user/programing/diveintocode-ml/Sprint16/ObjectDetection'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "sI_hXZmq2Pqa",
    "outputId": "73242145-f064-4b11-8a54-fa4b231b185a"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/qqwweee/keras-yolo3.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eNjDsbT_2Vux",
    "outputId": "56d6f461-e787-4ae5-9b63-bb21f2ec7245"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/user/programing/diveintocode-ml/Sprint16/ObjectDetection/keras-yolo3\n"
     ]
    }
   ],
   "source": [
    "cd keras-yolo3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !brew install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://pjreddie.com/media/files/yolov3.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ur2TqoCR2m8b"
   },
   "outputs": [],
   "source": [
    "#事前学習済みの重みをロード\n",
    "# !python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KkghfJJE5-9y",
    "outputId": "48a4093d-6e6c-48d4-94aa-f9be06c88d6d"
   },
   "outputs": [],
   "source": [
    "# !python train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4DUoR9ba5c8c",
    "outputId": "5c419136-0c4d-4097-8c56-d40728d988cc"
   },
   "outputs": [],
   "source": [
    "# !python yolo_video.py --image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M_JM21wu846G"
   },
   "source": [
    "## 【問題4】学習のためのファイルを作成\n",
    "新しいデータ（シンプソンズデータセット）を学習します。README.mdのTrainingの項を読み、シンプソンズデータセットを学習するために必要なファイルを作成してください。\n",
    "\n",
    "アノテーションファイルの形式が問題1の実装とは異なるため変換する必要があります。\n",
    "《アノテーションファイルの形式》\n",
    "image_file_path,xmin,ymin,xmax,ymax,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Sxs6vSI1DGCC",
    "outputId": "626138bc-a399-4a23-df07-296c67eca31b"
   },
   "outputs": [],
   "source": [
    "#simpsonの画像データをYOLOへ移動\n",
    "# !mv simpson_dataset keras-yolo3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sprint17_fastrcnn_yolo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
