{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変数の定義\n",
    "a = tf.constant(5)\n",
    "b = tf.constant(7)\n",
    "\n",
    "# aとbをたす。\n",
    "add = tf.add(a, b)\n",
    "\n",
    "# セッションスタート\n",
    "sess = tf.Session()\n",
    "\n",
    "# 計算開始\n",
    "out = sess.run(add)\n",
    "\n",
    "# セッションクローズ\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(out) # 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    output = sess.run(add)\n",
    "    print(output) # 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "a_n = np.array(5)\n",
    "b_n = np.array(7)\n",
    "output_n = np.add(a_n, b_n)\n",
    "print(output_n) # 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# これでもいける。\n",
    "with tf.Session() as sess:\n",
    "#     session start\n",
    "\n",
    "    output = sess.run(add)\n",
    "    print(output) # 12\n",
    "    \n",
    "#     sess.close()は不要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(5)\n",
    "b = tf.constant(7)\n",
    "add = tf.add(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_2:0\", shape=(), dtype=int32)\n",
      "Tensor(\"Add_1:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(a) # Tensor(\"Const:0\", shape=(), dtype=int32)\n",
    "print(add) # Tensor(\"Add:0\", shape=(), dtype=int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "c = tf.placeholder(tf.int32)\n",
    "d = tf.placeholder(tf.int32)\n",
    "add = tf.add(c, d)\n",
    "sess = tf.Session()\n",
    "output = sess.run(add, feed_dict={c:5, d:7})\n",
    "print(output) # 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "output = sess.run(add, feed_dict={c:20, d:32})\n",
    "print(output) # 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "add = a + b # tf.add(a, b)に等しい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(add) # ここに計算の実行コードを入れていく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_train = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y_train = np.array([[0],[0],[0],[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.placeholder(tf.float32, [None, 2])\n",
    "t = tf.placeholder(tf.float32, [None, 1])\n",
    "# 第二引数の[None,2]で行列の形を指定しています。\n",
    "# 2はデータの次元を表しています。Noneの部分はデータ数を表す部分です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([2,1]))\n",
    "b = tf.Variable(tf.zeros([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.sigmoid(tf.matmul(x, W) + b)\n",
    "cross_entropy = tf.reduce_sum(-t * tf.log(y) - (1 - t) * tf.log(1 - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.sign(y - 0.5), tf.sign(t - 0.5))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, Accuracy: 0.750000\n",
      "epoch: 100, Accuracy: 1.000000\n",
      "epoch: 200, Accuracy: 1.000000\n",
      "epoch: 300, Accuracy: 1.000000\n",
      "epoch: 400, Accuracy: 1.000000\n",
      "epoch: 500, Accuracy: 1.000000\n",
      "epoch: 600, Accuracy: 1.000000\n",
      "epoch: 700, Accuracy: 1.000000\n",
      "epoch: 800, Accuracy: 1.000000\n",
      "epoch: 900, Accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    sess.run(train_step, feed_dict={\n",
    "        x:x_train,\n",
    "        t:y_train\n",
    "    })\n",
    "# 100回ごとに正解率を表示\n",
    "    if epoch % 100 == 0:\n",
    "        acc_val = sess.run(\n",
    "            accuracy, feed_dict={\n",
    "                x:x_train,\n",
    "                t:y_train})\n",
    "        print ('epoch: %d, Accuracy: %f'\n",
    "               %(epoch, acc_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "[[1.9651403e-04]\n",
      " [4.9049813e-02]\n",
      " [4.9049813e-02]\n",
      " [9.3120378e-01]]\n"
     ]
    }
   ],
   "source": [
    "#学習結果が正しいか確認\n",
    "classified = sess.run(correct_prediction, feed_dict={\n",
    "    x:x_train,\n",
    "    t:y_train\n",
    "})\n",
    "#出力yの確認\n",
    "prob = sess.run(y, feed_dict={\n",
    "    x:x_train,\n",
    "    t:y_train\n",
    "})\n",
    "print(classified)\n",
    "# [[ True]\n",
    "# [ True]\n",
    "# [ True]\n",
    "# [ True]]\n",
    "print(prob)\n",
    "# [[  1.96514215e-04]\n",
    "# [  4.90498319e-02]\n",
    "# [  4.90498319e-02]\n",
    "# [  9.31203783e-01]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [[5.569955]\n",
      " [5.569955]]\n",
      "b: [-8.53458]\n"
     ]
    }
   ],
   "source": [
    "print('W:', sess.run(W))\n",
    "print('b:', sess.run(b))\n",
    "# W: [[ 5.5699544]\n",
    "# [ 5.5699544]]\n",
    "# b: [-8.53457928]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.      ]\n",
      " [ 5.569955]\n",
      " [ 5.569955]\n",
      " [11.13991 ]]\n"
     ]
    }
   ],
   "source": [
    "mat = tf.matmul(x, W)\n",
    "y = tf.sigmoid(mat + b)\n",
    "print(sess.run(mat, feed_dict={\n",
    "    x:x_train,\n",
    "    t:y_train\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習終わり"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】スクラッチを振り返る\n",
    "ここまでのスクラッチを振り返り、ディープラーニングを実装するためにはどのようなものが必要だったかを列挙してください。### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "層の構成を定義する。  \n",
    "層ごとに重みの保存と更新、  \n",
    "optimizer, activatre, loss, の指定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】スクラッチとTensorFlowの対応を考える\n",
    "以下のサンプルコードを見て、先ほど列挙した「ディープラーニングを実装するために必要なもの」がTensorFlowではどう実装されているかを確認してください。### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4   5            5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データセットの読み込み\n",
    "dataset_path =\"/Users/user/Downloads/school_dataset/Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm',\n",
       "       'Species'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 6)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 2.1500, val_loss : 2.9631, acc : 0.750, val_acc : 0.500\n",
      "Epoch 1, loss : 0.0029, val_loss : 0.2042, acc : 1.000, val_acc : 0.875\n",
      "Epoch 2, loss : 2.4616, val_loss : 2.6054, acc : 0.750, val_acc : 0.875\n",
      "Epoch 3, loss : 0.0017, val_loss : 0.7773, acc : 1.000, val_acc : 0.750\n",
      "Epoch 4, loss : 0.0428, val_loss : 1.3316, acc : 1.000, val_acc : 0.875\n",
      "Epoch 5, loss : 0.8070, val_loss : 3.3656, acc : 0.750, val_acc : 0.750\n",
      "Epoch 6, loss : 0.7674, val_loss : 2.3317, acc : 0.750, val_acc : 0.875\n",
      "Epoch 7, loss : 5.4344, val_loss : 10.8387, acc : 0.750, val_acc : 0.438\n",
      "Epoch 8, loss : 5.3382, val_loss : 6.1216, acc : 0.750, val_acc : 0.688\n",
      "Epoch 9, loss : 8.9175, val_loss : 18.2189, acc : 0.750, val_acc : 0.438\n",
      "Epoch 10, loss : 4.0489, val_loss : 5.4595, acc : 0.750, val_acc : 0.812\n",
      "Epoch 11, loss : 2.8851, val_loss : 7.1713, acc : 0.750, val_acc : 0.625\n",
      "Epoch 12, loss : 0.0000, val_loss : 2.0116, acc : 1.000, val_acc : 0.875\n",
      "Epoch 13, loss : 0.0000, val_loss : 0.0681, acc : 1.000, val_acc : 0.938\n",
      "Epoch 14, loss : 0.0000, val_loss : 0.0000, acc : 1.000, val_acc : 1.000\n",
      "Epoch 15, loss : 0.0000, val_loss : 0.4611, acc : 1.000, val_acc : 0.938\n",
      "Epoch 16, loss : 0.0000, val_loss : 0.0000, acc : 1.000, val_acc : 1.000\n",
      "Epoch 17, loss : 0.0000, val_loss : 0.0005, acc : 1.000, val_acc : 1.000\n",
      "Epoch 18, loss : 0.0000, val_loss : 0.0605, acc : 1.000, val_acc : 0.938\n",
      "Epoch 19, loss : 0.0000, val_loss : 0.0000, acc : 1.000, val_acc : 1.000\n",
      "Epoch 20, loss : 0.0000, val_loss : 0.0422, acc : 1.000, val_acc : 1.000\n",
      "Epoch 21, loss : 0.0000, val_loss : 0.0000, acc : 1.000, val_acc : 1.000\n",
      "Epoch 22, loss : 0.0000, val_loss : 0.0336, acc : 1.000, val_acc : 1.000\n",
      "Epoch 23, loss : 0.0000, val_loss : 0.0000, acc : 1.000, val_acc : 1.000\n",
      "Epoch 24, loss : 0.0000, val_loss : 0.1391, acc : 1.000, val_acc : 0.938\n",
      "Epoch 25, loss : 0.0000, val_loss : 0.0000, acc : 1.000, val_acc : 1.000\n",
      "Epoch 26, loss : 0.0000, val_loss : 0.0107, acc : 1.000, val_acc : 1.000\n",
      "Epoch 27, loss : 0.0000, val_loss : 0.0000, acc : 1.000, val_acc : 1.000\n",
      "Epoch 28, loss : 0.0000, val_loss : 0.0054, acc : 1.000, val_acc : 1.000\n",
      "Epoch 29, loss : 0.0000, val_loss : 0.0000, acc : 1.000, val_acc : 1.000\n",
      "Epoch 30, loss : 0.0000, val_loss : 0.0153, acc : 1.000, val_acc : 1.000\n",
      "Epoch 31, loss : 0.0000, val_loss : 0.0000, acc : 1.000, val_acc : 1.000\n",
      "Epoch 32, loss : 0.0000, val_loss : 0.0214, acc : 1.000, val_acc : 1.000\n",
      "Epoch 33, loss : 0.0000, val_loss : 0.0000, acc : 1.000, val_acc : 1.000\n",
      "Epoch 34, loss : 0.0000, val_loss : 0.0885, acc : 1.000, val_acc : 0.938\n",
      "Epoch 35, loss : 0.0000, val_loss : 0.0000, acc : 1.000, val_acc : 1.000\n",
      "Epoch 36, loss : 0.0000, val_loss : 0.2787, acc : 1.000, val_acc : 0.938\n",
      "Epoch 37, loss : 0.0000, val_loss : 0.0001, acc : 1.000, val_acc : 1.000\n",
      "Epoch 38, loss : 0.0000, val_loss : 0.1275, acc : 1.000, val_acc : 0.938\n",
      "Epoch 39, loss : 0.0000, val_loss : 0.0001, acc : 1.000, val_acc : 1.000\n",
      "Epoch 40, loss : 0.0000, val_loss : 0.1913, acc : 1.000, val_acc : 0.938\n",
      "Epoch 41, loss : 0.0000, val_loss : 0.0000, acc : 1.000, val_acc : 1.000\n",
      "Epoch 42, loss : 0.0000, val_loss : 0.1385, acc : 1.000, val_acc : 0.938\n",
      "Epoch 43, loss : 0.0000, val_loss : 0.0003, acc : 1.000, val_acc : 1.000\n",
      "Epoch 44, loss : 0.0000, val_loss : 0.5333, acc : 1.000, val_acc : 0.938\n",
      "Epoch 45, loss : 0.0000, val_loss : 1.7918, acc : 1.000, val_acc : 0.812\n",
      "Epoch 46, loss : 0.0000, val_loss : 2.4390, acc : 1.000, val_acc : 0.875\n",
      "Epoch 47, loss : 1.3364, val_loss : 6.0049, acc : 0.750, val_acc : 0.750\n",
      "Epoch 48, loss : 0.0206, val_loss : 4.0775, acc : 1.000, val_acc : 0.875\n",
      "Epoch 49, loss : 4.0529, val_loss : 9.9978, acc : 0.750, val_acc : 0.625\n",
      "test_acc : 0.600\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# データセットの読み込み\n",
    "# dataset_path =\"Iris.csv\"\n",
    "# df = pd.read_csv(dataset_path)\n",
    "\n",
    "# データフレームから条件抽出\n",
    "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "    \n",
    "    \n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 50\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "# 目的関数\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# 推定結果\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "層の構成を定義する  \n",
    "example_net(x)によって定義されている。  \n",
    "\n",
    "層ごとに重みの保存と更新  \n",
    "logitsを更新していると思われる。\n",
    "\n",
    "optimizer, activatre, loss, の指定  \n",
    "それぞれ、optimizer, layer, loss_opのところで定義されている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】3種類全ての目的変数を使用したIrisのモデルを作成\n",
    "Irisデータセットのtrain.csvの中で、目的変数Speciesに含まれる3種類全てを分類できるモデルを作成してください。### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4   5            5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データセットの読み込み\n",
    "dataset_path =\"/Users/user/Downloads/school_dataset/Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:X_train.shape,X_val.shape,X_test.shape(96, 4)(24, 4)(30, 4)\n",
      "y:y_train.shape,y_val.shape,y_test.shape(96, 3)(24, 3)(30, 3)\n",
      "Epoch 0, loss : 64.8698, val_loss : 146.6845, acc : 0.333, val_acc : 0.333\n",
      "Epoch 1, loss : 51.1941, val_loss : 69.7700, acc : 0.333, val_acc : 0.333\n",
      "Epoch 2, loss : 26.0945, val_loss : 35.4539, acc : 0.333, val_acc : 0.333\n",
      "Epoch 3, loss : 14.1754, val_loss : 3.9239, acc : 0.333, val_acc : 0.000\n",
      "Epoch 4, loss : 6.1843, val_loss : 0.2678, acc : 0.333, val_acc : 0.333\n",
      "Epoch 5, loss : 0.3980, val_loss : 0.1722, acc : 0.667, val_acc : 0.333\n",
      "Epoch 6, loss : 1.2356, val_loss : 0.5108, acc : 0.667, val_acc : 0.667\n",
      "Epoch 7, loss : 0.1478, val_loss : 0.0773, acc : 0.667, val_acc : 0.333\n",
      "Epoch 8, loss : 0.8547, val_loss : 0.4861, acc : 0.667, val_acc : 0.667\n",
      "Epoch 9, loss : 0.0662, val_loss : 0.0911, acc : 0.667, val_acc : 0.333\n",
      "Epoch 10, loss : 0.4529, val_loss : 0.3935, acc : 0.667, val_acc : 0.667\n",
      "Epoch 11, loss : 0.0893, val_loss : 0.2243, acc : 0.667, val_acc : 0.667\n",
      "Epoch 12, loss : 0.0889, val_loss : 0.2929, acc : 0.667, val_acc : 0.667\n",
      "Epoch 13, loss : 0.0728, val_loss : 0.3384, acc : 0.667, val_acc : 0.667\n",
      "Epoch 14, loss : 0.0216, val_loss : 0.2901, acc : 0.667, val_acc : 0.667\n",
      "Epoch 15, loss : 0.0267, val_loss : 0.3524, acc : 0.667, val_acc : 0.667\n",
      "Epoch 16, loss : 0.0147, val_loss : 0.3531, acc : 0.667, val_acc : 0.667\n",
      "Epoch 17, loss : 0.0148, val_loss : 0.4032, acc : 0.667, val_acc : 0.667\n",
      "Epoch 18, loss : 0.0109, val_loss : 0.4395, acc : 0.667, val_acc : 0.667\n",
      "Epoch 19, loss : 0.0064, val_loss : 0.4517, acc : 0.667, val_acc : 0.667\n",
      "Epoch 20, loss : 0.0055, val_loss : 0.4881, acc : 0.667, val_acc : 0.667\n",
      "Epoch 21, loss : 0.0039, val_loss : 0.5081, acc : 0.667, val_acc : 0.667\n",
      "Epoch 22, loss : 0.0029, val_loss : 0.5313, acc : 0.667, val_acc : 0.667\n",
      "Epoch 23, loss : 0.0020, val_loss : 0.5481, acc : 0.667, val_acc : 0.667\n",
      "Epoch 24, loss : 0.0014, val_loss : 0.5710, acc : 0.667, val_acc : 0.667\n",
      "Epoch 25, loss : 0.0010, val_loss : 0.5928, acc : 0.667, val_acc : 0.667\n",
      "Epoch 26, loss : 0.0008, val_loss : 0.6204, acc : 0.667, val_acc : 0.667\n",
      "Epoch 27, loss : 0.0006, val_loss : 0.6433, acc : 0.667, val_acc : 0.667\n",
      "Epoch 28, loss : 0.0005, val_loss : 0.6791, acc : 0.667, val_acc : 0.667\n",
      "Epoch 29, loss : 0.0003, val_loss : 0.6648, acc : 0.667, val_acc : 0.667\n",
      "Epoch 30, loss : 0.0007, val_loss : 0.7614, acc : 0.667, val_acc : 0.667\n",
      "Epoch 31, loss : 0.0001, val_loss : 0.5766, acc : 1.000, val_acc : 0.667\n",
      "Epoch 32, loss : 0.0067, val_loss : 1.0170, acc : 0.667, val_acc : 0.667\n",
      "Epoch 33, loss : 0.0000, val_loss : 0.5301, acc : 1.000, val_acc : 0.667\n",
      "Epoch 34, loss : 0.0034, val_loss : 0.9984, acc : 0.667, val_acc : 0.667\n",
      "Epoch 35, loss : 0.0000, val_loss : 0.5773, acc : 1.000, val_acc : 0.667\n",
      "Epoch 36, loss : 0.0014, val_loss : 0.9614, acc : 0.667, val_acc : 0.667\n",
      "Epoch 37, loss : 0.0000, val_loss : 0.6067, acc : 1.000, val_acc : 0.667\n",
      "Epoch 38, loss : 0.0023, val_loss : 1.0566, acc : 0.667, val_acc : 0.667\n",
      "Epoch 39, loss : 0.0000, val_loss : 0.6486, acc : 1.000, val_acc : 0.667\n",
      "Epoch 40, loss : 0.0022, val_loss : 1.0907, acc : 0.667, val_acc : 0.667\n",
      "Epoch 41, loss : 0.0000, val_loss : 0.6743, acc : 1.000, val_acc : 0.667\n",
      "Epoch 42, loss : 0.0027, val_loss : 1.1453, acc : 0.667, val_acc : 0.667\n",
      "Epoch 43, loss : 0.0000, val_loss : 0.6739, acc : 1.000, val_acc : 0.667\n",
      "Epoch 44, loss : 0.0040, val_loss : 1.2179, acc : 0.667, val_acc : 0.667\n",
      "Epoch 45, loss : 0.0000, val_loss : 0.6583, acc : 1.000, val_acc : 0.667\n",
      "Epoch 46, loss : 0.0062, val_loss : 1.2924, acc : 0.667, val_acc : 0.667\n",
      "Epoch 47, loss : 0.0000, val_loss : 0.6592, acc : 1.000, val_acc : 0.667\n",
      "Epoch 48, loss : 0.0069, val_loss : 1.3306, acc : 0.667, val_acc : 0.667\n",
      "Epoch 49, loss : 0.0000, val_loss : 0.7031, acc : 1.000, val_acc : 0.667\n",
      "test_acc : 1.000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを\"\"\"'3'\"\"\"値分類する\n",
    "\"\"\"\n",
    "# データセットの読み込み\n",
    "dataset_path =\"/Users/user/Downloads/school_dataset/Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# データフレームから条件抽出\n",
    "# 三種類抽出\n",
    "df = df[(df[\"Species\"] == \"Iris-setosa\")|\n",
    "    (df[\"Species\"] == \"Iris-versicolor\")|\n",
    "    (df[\"Species\"] == \"Iris-virginica\")]\n",
    "\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "\n",
    "# ラベルを数値に変換\n",
    "\"\"\"変更点\"\"\"\n",
    "y[y=='Iris-setosa'] = 0\n",
    "y[y=='Iris-versicolor'] = 1\n",
    "y[y=='Iris-virginica'] = 2\n",
    "\n",
    "# ワンホット化\n",
    "\"\"\"変更点\"\"\"\n",
    "y = pd.get_dummies(y)\n",
    "\n",
    "# array化\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "\n",
    "# yデータをint化\n",
    "\"\"\"変更点\"\"\"\n",
    "y = y.astype(np.int)\n",
    "\n",
    "# train, val, testに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "print('X:X_train.shape,X_val.shape,X_test.shape{}{}{}'.format(X_train.shape,X_val.shape,X_test.shape))\n",
    "print('y:y_train.shape,y_val.shape,y_test.shape{}{}{}'.format(y_train.shape,y_val.shape,y_test.shape))\n",
    "    \n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 50\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "\"\"\"変更点\"\"\"\n",
    "n_classes = 3\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "\"\"\"変更点\"\"\"\n",
    "Y = tf.placeholder(\"int64\", [None, n_classes])\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "\n",
    "# 目的関数\n",
    "\"\"\"変更点\"\"\"\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 推定結果\n",
    "\"\"\"変更点\"\"\"\n",
    "correct_pred = tf.equal(tf.argmax(Y, 0), tf.argmax(tf.nn.softmax(logits),0))\n",
    "\n",
    "# 指標値計算\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float64))\n",
    "\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    \n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題4】House Pricesのモデルを作成\n",
    "回帰問題のデータセットであるHouse Pricesを使用したモデルを作成してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:X_train.shape,X_val.shape,X_test.shape(934, 2)(234, 2)(292, 2)\n",
      "y:y_train.shape,y_val.shape,y_test.shape(934, 1)(234, 1)(292, 1)\n",
      "Epoch 0, train_loss : 29.6925, val_loss : 36.9196, train_RMSE : 7.706, val_RMSE : 8.593\n",
      "Epoch 1, train_loss : 30.7340, val_loss : 29.4532, train_RMSE : 7.840, val_RMSE : 7.675\n",
      "Epoch 2, train_loss : 4.9555, val_loss : 5.5591, train_RMSE : 3.148, val_RMSE : 3.334\n",
      "Epoch 3, train_loss : 0.0830, val_loss : 0.6320, train_RMSE : 0.407, val_RMSE : 1.124\n",
      "Epoch 4, train_loss : 0.1437, val_loss : 0.2188, train_RMSE : 0.536, val_RMSE : 0.661\n",
      "Epoch 5, train_loss : 0.1258, val_loss : 0.1797, train_RMSE : 0.502, val_RMSE : 0.600\n",
      "Epoch 6, train_loss : 0.0986, val_loss : 0.1478, train_RMSE : 0.444, val_RMSE : 0.544\n",
      "Epoch 7, train_loss : 0.0788, val_loss : 0.1236, train_RMSE : 0.397, val_RMSE : 0.497\n",
      "Epoch 8, train_loss : 0.0653, val_loss : 0.1092, train_RMSE : 0.361, val_RMSE : 0.467\n",
      "Epoch 9, train_loss : 0.0557, val_loss : 0.0997, train_RMSE : 0.334, val_RMSE : 0.447\n",
      "Epoch 10, train_loss : 0.0477, val_loss : 0.0910, train_RMSE : 0.309, val_RMSE : 0.427\n",
      "Epoch 11, train_loss : 0.0439, val_loss : 0.0839, train_RMSE : 0.296, val_RMSE : 0.410\n",
      "Epoch 12, train_loss : 0.0454, val_loss : 0.0794, train_RMSE : 0.301, val_RMSE : 0.398\n",
      "Epoch 13, train_loss : 0.0531, val_loss : 0.0778, train_RMSE : 0.326, val_RMSE : 0.394\n",
      "Epoch 14, train_loss : 0.0652, val_loss : 0.0794, train_RMSE : 0.361, val_RMSE : 0.399\n",
      "Epoch 15, train_loss : 0.0774, val_loss : 0.0823, train_RMSE : 0.394, val_RMSE : 0.406\n",
      "Epoch 16, train_loss : 0.0887, val_loss : 0.0859, train_RMSE : 0.421, val_RMSE : 0.415\n",
      "Epoch 17, train_loss : 0.0989, val_loss : 0.0902, train_RMSE : 0.445, val_RMSE : 0.425\n",
      "Epoch 18, train_loss : 0.1042, val_loss : 0.0926, train_RMSE : 0.457, val_RMSE : 0.430\n",
      "Epoch 19, train_loss : 0.1021, val_loss : 0.0911, train_RMSE : 0.452, val_RMSE : 0.427\n",
      "Epoch 20, train_loss : 0.0941, val_loss : 0.0863, train_RMSE : 0.434, val_RMSE : 0.415\n",
      "Epoch 21, train_loss : 0.0843, val_loss : 0.0807, train_RMSE : 0.411, val_RMSE : 0.402\n",
      "Epoch 22, train_loss : 0.0747, val_loss : 0.0756, train_RMSE : 0.387, val_RMSE : 0.389\n",
      "Epoch 23, train_loss : 0.0674, val_loss : 0.0716, train_RMSE : 0.367, val_RMSE : 0.379\n",
      "Epoch 24, train_loss : 0.0628, val_loss : 0.0693, train_RMSE : 0.355, val_RMSE : 0.372\n",
      "Epoch 25, train_loss : 0.0606, val_loss : 0.0681, train_RMSE : 0.348, val_RMSE : 0.369\n",
      "Epoch 26, train_loss : 0.0606, val_loss : 0.0679, train_RMSE : 0.348, val_RMSE : 0.368\n",
      "Epoch 27, train_loss : 0.0605, val_loss : 0.0675, train_RMSE : 0.348, val_RMSE : 0.367\n",
      "Epoch 28, train_loss : 0.0595, val_loss : 0.0667, train_RMSE : 0.345, val_RMSE : 0.365\n",
      "Epoch 29, train_loss : 0.0571, val_loss : 0.0655, train_RMSE : 0.338, val_RMSE : 0.362\n",
      "Epoch 30, train_loss : 0.0529, val_loss : 0.0636, train_RMSE : 0.325, val_RMSE : 0.357\n",
      "Epoch 31, train_loss : 0.0468, val_loss : 0.0611, train_RMSE : 0.306, val_RMSE : 0.350\n",
      "Epoch 32, train_loss : 0.0393, val_loss : 0.0589, train_RMSE : 0.280, val_RMSE : 0.343\n",
      "Epoch 33, train_loss : 0.0315, val_loss : 0.0579, train_RMSE : 0.251, val_RMSE : 0.340\n",
      "Epoch 34, train_loss : 0.0254, val_loss : 0.0593, train_RMSE : 0.225, val_RMSE : 0.344\n",
      "Epoch 35, train_loss : 0.0219, val_loss : 0.0633, train_RMSE : 0.209, val_RMSE : 0.356\n",
      "Epoch 36, train_loss : 0.0213, val_loss : 0.0698, train_RMSE : 0.206, val_RMSE : 0.374\n",
      "Epoch 37, train_loss : 0.0226, val_loss : 0.0779, train_RMSE : 0.213, val_RMSE : 0.395\n",
      "Epoch 38, train_loss : 0.0260, val_loss : 0.0873, train_RMSE : 0.228, val_RMSE : 0.418\n",
      "Epoch 39, train_loss : 0.0312, val_loss : 0.0987, train_RMSE : 0.250, val_RMSE : 0.444\n",
      "Epoch 40, train_loss : 0.0384, val_loss : 0.1123, train_RMSE : 0.277, val_RMSE : 0.474\n",
      "Epoch 41, train_loss : 0.0476, val_loss : 0.1280, train_RMSE : 0.309, val_RMSE : 0.506\n",
      "Epoch 42, train_loss : 0.0570, val_loss : 0.1433, train_RMSE : 0.338, val_RMSE : 0.535\n",
      "Epoch 43, train_loss : 0.0641, val_loss : 0.1543, train_RMSE : 0.358, val_RMSE : 0.555\n",
      "Epoch 44, train_loss : 0.0652, val_loss : 0.1560, train_RMSE : 0.361, val_RMSE : 0.559\n",
      "Epoch 45, train_loss : 0.0579, val_loss : 0.1448, train_RMSE : 0.340, val_RMSE : 0.538\n",
      "Epoch 46, train_loss : 0.0460, val_loss : 0.1256, train_RMSE : 0.303, val_RMSE : 0.501\n",
      "Epoch 47, train_loss : 0.0354, val_loss : 0.1071, train_RMSE : 0.266, val_RMSE : 0.463\n",
      "Epoch 48, train_loss : 0.0291, val_loss : 0.0949, train_RMSE : 0.241, val_RMSE : 0.436\n",
      "Epoch 49, train_loss : 0.0257, val_loss : 0.0877, train_RMSE : 0.227, val_RMSE : 0.419\n",
      "test_RMSE : 0.393\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TensorFlowで実装したニューラルネットワークを使いHouse Pricesのモデルを作成\n",
    "\"\"\"\n",
    "# データセットの読み込み\n",
    "\"\"\"変更点\"\"\"\n",
    "dataset_path =\"/Users/user/Downloads/house-prices-advanced-regression-techniques/train.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# 必要な分抽出\n",
    "\"\"\"変更点\"\"\"\n",
    "y = df[\"SalePrice\"]\n",
    "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
    "\n",
    "### yに次元追加\n",
    "\"\"\"変更点\"\"\"\n",
    "y = np.array(y)[:, np.newaxis]\n",
    "X = np.array(X)\n",
    "\n",
    "### 対数変換\n",
    "\"\"\"変更点\"\"\"\n",
    "y = np.log(y)\n",
    "X = np.log(X)\n",
    "\n",
    "# train, val, testに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# 形状確認\n",
    "print('X:X_train.shape,X_val.shape,X_test.shape{}{}{}'.format(X_train.shape,X_val.shape,X_test.shape))\n",
    "print('y:y_train.shape,y_val.shape,y_test.shape{}{}{}'.format(y_train.shape,y_val.shape,y_test.shape))\n",
    "\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 50\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "\"\"\"変更点\"\"\"\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "# 層構造の作成\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "\n",
    "# 目的関数 平均２乗誤差\n",
    "\"\"\"変更点\"\"\"\n",
    "loss_op = tf.reduce_mean(tf.square(Y - logits))/2\n",
    "\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 指標値計算 RMSE\n",
    "\"\"\"変更点\"\"\"\n",
    "RMSE = tf.math.sqrt(tf.reduce_mean(tf.square(Y - logits)))\n",
    "\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "        # エポックごとにループ\n",
    "    for epoch in range(num_epochs):\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "\n",
    "            # ミニバッチごとにループ\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            train_loss, train_RMSE = sess.run([loss_op, RMSE], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += train_loss\n",
    "            total_acc += train_RMSE\n",
    "\n",
    "        # 平均化\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "\n",
    "        # 学習用、検証用\n",
    "        val_loss, val_RMSE = sess.run([loss_op, RMSE], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, train_loss : {:.4f}, val_loss : {:.4f}, train_RMSE : {:.3f}, val_RMSE : {:.3f}\".format(epoch, train_loss, val_loss, train_RMSE, val_RMSE))\n",
    "    \n",
    "    # テスト用\n",
    "    test_RMSE = sess.run(RMSE, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_RMSE : {:.3f}\".format(test_RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題5】MNISTのモデルを作成\n",
    "ニューラルネットワークのスクラッチで使用したMNISTを分類するモデルを作成してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import MNIST data\n",
    "# data from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/input_data.py\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data\n",
      "(55000, 784) (55000, 10)\n",
      "test_data\n",
      "(10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    X, y = mnist.train.images, mnist.train.labels\n",
    "    print('train_data')\n",
    "    print(X.shape, y.shape )\n",
    "    X, y = mnist.test.images, mnist.test.labels\n",
    "    print('test_data')\n",
    "    print(X.shape, y.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-158-520e7be3b3ba>:15: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-158-520e7be3b3ba>:71: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Step 1, Minibatch Loss= 67263.4453, Training Accuracy= 0.227\n",
      "Step 10, Minibatch Loss= 32258.7383, Training Accuracy= 0.242\n",
      "Step 20, Minibatch Loss= 8178.3857, Training Accuracy= 0.562\n",
      "Step 30, Minibatch Loss= 6666.8286, Training Accuracy= 0.617\n",
      "Step 40, Minibatch Loss= 4348.5239, Training Accuracy= 0.719\n",
      "Step 50, Minibatch Loss= 2866.0537, Training Accuracy= 0.812\n",
      "Step 60, Minibatch Loss= 3420.3525, Training Accuracy= 0.805\n",
      "Step 70, Minibatch Loss= 2918.9421, Training Accuracy= 0.836\n",
      "Step 80, Minibatch Loss= 2042.1908, Training Accuracy= 0.875\n",
      "Step 90, Minibatch Loss= 2133.1270, Training Accuracy= 0.812\n",
      "Step 100, Minibatch Loss= 1363.2439, Training Accuracy= 0.914\n",
      "Step 110, Minibatch Loss= 2156.1465, Training Accuracy= 0.891\n",
      "Step 120, Minibatch Loss= 1440.9493, Training Accuracy= 0.922\n",
      "Step 130, Minibatch Loss= 1678.6396, Training Accuracy= 0.883\n",
      "Step 140, Minibatch Loss= 1569.9077, Training Accuracy= 0.906\n",
      "Step 150, Minibatch Loss= 1575.5513, Training Accuracy= 0.906\n",
      "Step 160, Minibatch Loss= 1927.6179, Training Accuracy= 0.891\n",
      "Step 170, Minibatch Loss= 447.2089, Training Accuracy= 0.945\n",
      "Step 180, Minibatch Loss= 1397.3462, Training Accuracy= 0.914\n",
      "Step 190, Minibatch Loss= 1372.2061, Training Accuracy= 0.914\n",
      "Step 200, Minibatch Loss= 1817.0194, Training Accuracy= 0.898\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.94140625\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Convolutional Neural Network.\n",
    "Build and train a convolutional neural network with TensorFlow.\n",
    "This example is using the MNIST database of handwritten digits\n",
    "(http://yann.lecun.com/exdb/mnist/)\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 200\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [None, num_input])\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)\n",
    "\n",
    "\n",
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "    # Reshape to match picture format [Height x Width x Channel]\n",
    "    # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, num_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "logits = conv_net(X, weights, biases, keep_prob)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: 0.8})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y,\n",
    "                                                                 keep_prob: 1.0})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 256 MNIST test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: mnist.test.images[:256],\n",
    "                                      Y: mnist.test.labels[:256],\n",
    "                                      keep_prob: 1.0}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
